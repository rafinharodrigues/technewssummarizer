{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "462197aa-965b-4265-9c00-3c6501d6f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a7162acd-5c3a-41de-bf49-4b399ee2a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cb413624-d3a8-4d9e-bf72-33990338d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website_link:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "    internal_links: list\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        \n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        \n",
    "        self.internal_links = []\n",
    "        domain = urlparse(url).netloc\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            link = urljoin(url, a['href'])  \n",
    "            if urlparse(link).netloc == domain:  \n",
    "                self.internal_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11277fcf-20a3-4543-9935-1e84674213c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website_text:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "30ded90a-a485-4a89-b359-81d5cd483b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... https://techcrunch.com/\n",
      "Loading... https://www.theverge.com/\n",
      "Loading... https://www.wired.com/\n",
      "Loading... https://arstechnica.com/\n",
      "Loading... https://gizmodo.com/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "weblist = ['https://techcrunch.com/', 'https://www.theverge.com/',\n",
    "           'https://www.wired.com/', 'https://arstechnica.com/',\n",
    "           'https://gizmodo.com/']\n",
    "final_text = {}\n",
    "\n",
    "%timeit\n",
    "for site in weblist:\n",
    "    print(f\"Loading... {site}\")\n",
    "    ed = Website_link(site)\n",
    "    internal_links = f'{ed.internal_links}'\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': (\n",
    "                    'You will return only the links which contain '\n",
    "                    'a news headline. You will return only the top 3 '\n",
    "                    'news that you consider relevant in tech. You will '\n",
    "                    'return each of the headlines and the links in a '\n",
    "                    'python dict. You will only return the dict without the '\n",
    "                    'python format. The python dict needs to be in this '\n",
    "                    'sample format: {\"1\": {\"headline\":\"...\",\"link\":\"...\"},\"2\": {\"headline\":\"...\",\"link\":\"...\"}}'\n",
    "                )\n",
    "            },\n",
    "            {'role': 'user', 'content': internal_links}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    links = response.choices[0].message.content\n",
    "    links = json.loads(links)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in links:\n",
    "        result = Website_text(links[i]['link'])\n",
    "        result_text = result.text\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': (\n",
    "                        'This is a newspaper news. You will resume the '\n",
    "                        'text into 1 paragraph. You will write a 3 minutes '\n",
    "                        'text talking about this news like you are a YouTuber '\n",
    "                        'news presenter. You will return the resume paragraph '\n",
    "                        'and the 3 minutes text in a python dict \"resume\" and \"text\". You will only return the dict without the '\n",
    "                        'python format.'\n",
    "                    )\n",
    "                },\n",
    "                {'role': 'user', 'content': result_text}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response_text = json.loads(response.choices[0].message.content)\n",
    "        results.append({links[i]['headline']: [response_text, {'url': links[i]['link']}]})\n",
    "\n",
    "    for item in results:\n",
    "        final_text.update(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5ef15fa1-7def-47af-b6df-8fc4b375c2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h1>Jon McNeill's insights on Tesla's robotaxi and why EV startups fail</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://techcrunch.com/2024/10/27/jon-mcneills-insights-on-teslas-robotaxi-and-why-ev-startups-fail/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jon McNeill, former Tesla and Lyft executive, shared insights at the World Business Forum about Tesla's automation ambitions for its robotaxi, emphasizing a surprising lack of substantial developments during its recent reveal. He questioned Tesla's reliance solely on cameras for self-driving capabilities, suggesting that lidar technology might be necessary for safety. Moreover, he discussed the broader challenges facing EV startups, noting that strong leadership is essential for navigating the complexities of car manufacturing, as evidenced by the struggles of companies like Fisker and Rivian.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we've got some intriguing insights from Jon McNeill, the former Tesla and Lyft executive, who recently made waves at the World Business Forum discussing Tesla's ambitious plans for their robotaxi. So, what did he say? Well, after the much-anticipated reveal of the Tesla robotaxi prototypes, which turned out to be just 20 of them, McNeill expressed his surprise that there wasn't more concrete information shared. You know, many were expecting significant breakthroughs that would illustrate Tesla’s journey from Level 2 to Level 4 automation, especially with competitors like Waymo already ahead in the game. But it appears, at least for now, Tesla is still working on its game plan. McNeill believes that while the approach of simplifying processes can be effective, Tesla’s decision to solely use cameras for its self-driving technology might not be enough. He raised a valid point: human drivers have two eyes, and if a vehicle has eight, it should theoretically do better, but visibility can still be compromised by rain, fog, darkness, and more. That's where lidar technology could play a crucial role. Given the stakes involved in safety, McNeill argued that cutting corners isn't the way to go. He also touched on why many EV startups are facing significant challenges, noting that it's incredibly tough to manufacture cars from the ground up. Companies like Fisker and Rivian are struggling with mounting losses, and McNeill suggests that crazy yet effective leadership is necessary to push through those difficult times. So, there you have it! Tesla's path toward a fully autonomous vehicle is proving to be a rocky one, and the question remains: can they adapt in time to keep up? Drop your thoughts below, and don’t forget to like and subscribe for more tech updates!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>The next Mac mini might be nearly as small as an Apple TV</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://techcrunch.com/2024/10/27/the-next-mac-mini-might-be-nearly-as-small-as-an-apple-tv/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple is set to release an upgraded Mac mini that will be nearly the size of an Apple TV and feature enhanced capabilities, including support for ray tracing graphics. This update, reported by Bloomberg's Mark Gurman, indicates that the new models will arrive alongside other Apple devices, such as a 24-inch iMac and updated MacBook Pros, all powered by Apple's M4 chip, promising to be among the most impressive offerings using the company's in-house silicon.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we’re diving into some exciting news from Apple that you won’t want to miss. According to Bloomberg’s very own Mark Gurman, Apple is gearing up to launch a new Mac mini this week, and it’s looking to be a game-changer! So, what’s the scoop? This new Mac mini is reportedly going to be nearly as small as an Apple TV. Yes, you heard that right, folks! Imagine a Mac that’s just a compact little gray box, perfectly sized to fit into your tech setup without dominating your desk. But that’s not all! This new model isn’t just about size; it’s set to include some impressive features, such as two ports in the front and additional ports in the back for some models. But the highlight is that it will support ray tracing graphics for the first time, marking a significant leap in performance. Gurman’s sources suggest that this could be the most impressive Mac yet that utilizes Apple’s own silicon. And if you were thinking, 'What else is Apple launching with this?'—we’ve got some more juicy details! Alongside the new Mac mini, Apple is expected to unveil a line-up that includes a 24-inch iMac, as well as upgraded MacBook Pro models, both 16-inch and 14-inch, all featuring the brand-new M4 chip. How crazy is that? And wait, there's even more! There are whispers of a rumored smart home display from Apple that’s about the size of two iPhones side by side, complete with an innovative design inspired by the iMac G4. So, if you’re a fan of Apple and the tech world in general, keep your eyes peeled for these announcements! Don't forget to like, subscribe, and hit that notification bell for all the latest updates from the tech world. We’ll keep you posted on everything once these devices hit the market. Until next time, stay tech-savvy!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>OpenAI's Whisper transcription tool has hallucination issues, researchers say</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://techcrunch.com/2024/10/26/openais-whisper-transcription-tool-has-hallucination-issues-researchers-say/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Researchers are raising alarm bells over OpenAI’s Whisper transcription tool, highlighting significant issues with hallucinations in its outputs. Reports indicate that transcriptions often include fabricated details, such as erroneous medical advice and inappropriate racial commentary, raising concerns particularly in sensitive environments like hospitals. Studies show that hallucinations were found in approximately 80% of the audio transcriptions analyzed. Despite OpenAI's ongoing efforts to improve accuracy, the implications of these hallucinations could be dire, especially as Whisper is increasingly adopted in critical fields.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today we've got a major headline from the AI world that has tech enthusiasts and industry professionals buzzing. We're talking about OpenAI’s Whisper transcription tool and some alarming concerns researchers are raising regarding its reliability. You wouldn’t think that a transcription tool would struggle with accuracy since it's supposed to convert spoken words directly into text. However, according to reports from the Associated Press, researchers have found that Whisper is not just struggling, it’s basically making things up! Yes, you heard that right—these so-called 'hallucinations' are creeping into its transcriptions. We’re talking about fabricated content that ranges from inappropriate racial comments to completely made-up medical treatments that could have serious implications if used in hospitals or healthcare settings! A researcher from the University of Michigan observed that a staggering eight out of ten transcriptions contained these hallucinations. It doesn’t end there; other developers have reported similar findings, with one creator noting that nearly all of the 26,000 transcriptions they produced with Whisper featured hallucinated content. OpenAI, while acknowledging these issues, insists they're continuously working on improving the tool's accuracy. They even emphasize that their usage policies prohibit the use of Whisper in high-stakes environments. But given the current state of things, it’s clear there's a lot of room for improvement before we can trust these AI tools, especially in sensitive areas like healthcare. So, what do you guys think about this? Is this just a growing pain in the world of AI, or is it a more serious issue that needs a closer look? Let me know your thoughts in the comments! If you enjoyed this video, don’t forget to like and subscribe for more updates on the latest in tech and AI. Catch you in the next one!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Apple Mac Mini M4 redesign preview</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.theverge.com/2024/10/27/24278827/apple-mac-mini-m4-redesign-preview>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple is set to unveil a significantly redesigned Mac Mini, expected to feature the powerful M4 chip and a compact design that rivals the size of an Apple TV. This new model promises advanced connectivity options, including five USB-C ports, and aims to enhance user experience with improvements like front-facing ports and compatibility with macOS Sequoia. The revamped Mini continues to be an affordable, versatile option in Apple's lineup, catering to users who want a reliable Mac experience without the extravagance of higher-end models.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey guys, welcome back to the channel! Today, we have some exciting news from Apple that’s sure to get tech fans buzzing. The company is gearing up to announce a revamped Mac Mini, and trust me when I say, it’s not your average upgrade; this one’s groundbreaking! If you remember the original Mac Mini that debuted back in 2005, it was marketed as a compact and affordable desktop solution. Well, fast forward to today, and we’re about to witness a remarkable evolution of that design. The latest reports, particularly from Bloomberg's Mark Gurman, suggest that the new M4-powered Mac Mini is shrinking down, potentially to the size of an Apple TV! Imagine that! But it won’t just be smaller; it’ll pack a punch with impressive specs. One of the standout features is that it will include five USB-C ports, which is a game changer for anyone who has struggled with the backside ports for years. Finally, no more awkwardly reaching around the back to plug in devices! What’s more, the new model is set to run on macOS Sequoia, bringing with it exciting features like iPhone Mirroring and window tiling—talk about a smooth user experience! The Mac Mini has always held a special place in Apple’s lineup, considered the unsung hero for its affordability and capability. Now, with this exciting redesign, it looks like the Mini is ready to take center stage once again. Many in the tech community are thrilled, including myself, about what this new model will offer. If the stars align, this could be a top choice for those looking to jump into the Mac ecosystem without breaking the bank. My only concern? Will Apple keep it from overshadowing the Mac Studio? We’ll just have to wait and see! So, what do you think about the upcoming Mac Mini? Are you excited? Let me know in the comments, and don't forget to subscribe for more tech updates!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Google next Gemini AI model to debut in December</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.theverge.com/2024/10/25/24279600/google-next-gemini-ai-model-openai-december>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a competitive landscape of AI development, both OpenAI and Google are set to unveil their latest models in December, with OpenAI planning a phased rollout of its successor to GPT-4 while Google aims for a wider release of its Gemini 2.0 model. However, sources suggest that Google's Gemini 2.0 may not achieve the anticipated performance improvements, reflecting a broader trend among major AI players like xAI, Meta, and Anthropic, who are all racing to introduce new advancements in AI technology.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we have some exciting news from the tech world that’s stirring up quite a buzz, and it revolves around the fierce competition in AI development. December is shaping up to be a dramatic month for AI enthusiasts as both OpenAI and Google plan to unveil their latest models. That’s right, folks! OpenAI is gearing up to launch a successor to GPT-4, and they’re doing a phased rollout starting with their business partners. Meanwhile, Google is diving into the fray with its highly anticipated Gemini 2.0 model, aiming for a broader public release. But hold on, it's not all smooth sailing over at Google. Insider sources are suggesting that Gemini 2.0 might not meet the high performance expectations that the team led by Demis Hassabis had hoped for. This fits into a larger narrative we’re seeing across the industry with other major players like xAI, Meta, and Anthropic all racing to debut their next frontier models as well. Is there a chance we could witness some interesting new capabilities from Google despite these performance challenges? Only time will tell! What’s clear is that the tech race is heating up, and it's definitely going to be an exciting showdown between these AI giants. So, what do you think? Will Gemini 2.0 take the crown, or will GPT-4's successor steal the spotlight? Drop your thoughts in the comments below, and don't forget to like and subscribe for more updates in the tech world!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Siri integrates ChatGPT features in iOS 18.2 developer beta</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.theverge.com/2024/10/25/24278716/siri-chat-gpt-ios-18-2-developer-beta-apple-intelligence>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Siri's latest upgrade in the iOS 18.2 developer beta integrates ChatGPT, transforming Siri from a basic search tool into a more intelligent assistant capable of handling complex queries. While users can choose whether or not Siri will consult ChatGPT for responses, the integration enhances Siri's functionality significantly. Additional features like Visual Intelligence allow users to analyze images with AI assistance. However, despite the improvements, there are still issues with AI inaccuracies, and users are advised to treat AI-generated answers as starting points rather than definitive facts.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey there, tech fans! Exciting news in the Apple world as Siri receives what could be its most significant upgrade yet with the iOS 18.2 developer beta, which is set to integrate ChatGPT directly into our beloved digital assistant! This means, folks, that Siri is no longer just your average search tool; it's stepping up its game to become a far more intelligent assistant. So, what does this really mean for you? Well, for starters, Siri will now be able to handle complex questions that previous versions could only dream of answering. Instead of just saying, 'Let me Google that for you,' it’s now 'Let me ChatGPT that for you.' But don't worry, Siri will ask for your confirmation first before passing any queries over to ChatGPT, which I initially thought was a great feature. However, after a bit of hands-on experience, I found myself just wanting the answers quicker, so I ended up turning that feature off! And here's the kicker—while Siri still can manage straightforward questions independently, it now has the capability to draw from ChatGPT for more intricate inquiries, like cocktail recipes! Need to know what to mix up with whiskey and lemon juice? Siri's got you covered with an actual list and descriptions, which is a huge upgrade from just suggesting you make a whiskey sour. Now, let’s talk about Visual Intelligence! This feature allows iPhone 16 users to snap a pic and have ChatGPT analyze it or even perform a Google image search—like having your own personal assistant right at your fingertips. Although, I did notice it has some stumbles, like inventing details that weren't actually in the images. Who knew AI could spin a tale like that? Apple’s even included strong privacy protections, assuring users that their requests won't be stored or used to train AI models unless you’ve logged in to your OpenAI account. Folks, this is a big step for Siri! While it's clear there are still some issues with accuracy—like that time it totally butchered a joke from a Garfield comic—the integration of ChatGPT is a leap forward for Siri’s capabilities. We're told that more advanced features will roll out in the future, but for now, get ready to embrace a smarter, yet slightly flawed, Siri experience. So, are you excited about this upgrade? Comment below and let’s chat about how you think Siri will change the way you interact with your iPhone!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Microsoft Envisions Every Screen as an Xbox. How’s That Going So Far?</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.wired.com/story/microsoft-envisions-every-screen-as-an-xbox-hows-that-going-so-far/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Microsoft's vision of \"every screen as an Xbox\" through Xbox Cloud Gaming is challenged by latency issues and the necessity of physical consoles, despite advancements in cloud technology. With the recent mild refresh of the Xbox Series X, users grapple with whether local consoles or cloud gaming better serve their needs, especially as the gaming ecosystem continues to diversify, underscoring the Xbox's ongoing relevance, albeit with room for improvement.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today we're diving into some really exciting news from the world of gaming, specifically around Microsoft and their ambitious vision for the Xbox. So, recently, Microsoft Gaming CEO Phil Spencer shared this bold idea that he envisions a future where every screen can be an Xbox. Sounds fantastic, right? With Xbox Cloud Gaming, you can technically stream your favorite games anywhere you want—be it on your laptop, phone, or even a fancy TV—making gaming more accessible than ever before. However, this vision faces some hiccups!\n",
      "\n",
      "I recently tried testing out the latest Xbox Series X while exploring how viable it is to play amazing games like Starfield through cloud gaming on devices like the Amazon Fire TV Stick. And let me tell you, while the technology is impressive, the reality can be a bit rough! Latency issues pop up, especially in fast-paced games where milliseconds matter. Imagine trying to dodge bullets in a game while your controller feels like it’s in slow motion. That's not just frustrating; it's nearly unplayable!\n",
      "\n",
      "Interestingly, even with high-speed internet access, the lag can turn an action-packed moment into a game of patience. It makes you wonder if the console really has a place in this cloud-centric future. As it stands, though, owning a console still feels essential for serious gamers who crave that instant response and local advantages.\n",
      "\n",
      "And speaking of consoles, the Xbox Series X just got a refresh! While it offers slightly more storage, it lacks key upgrades, making many players question if it's worth the hype—especially if you already own the original. Let's be real; most people may find just sticking to their current consoles or gaming PCs more practical, especially considering Xbox Game Pass is thriving and giving gamers access to a treasure trove of games without committing to more hardware.\n",
      "\n",
      "At the end of the day, the Xbox ecosystem is certainly growing, and Microsoft is making strides in the gaming world, but with more options than ever, gamers need to decide if they want to embrace this vision or stick to traditional gaming setups. So, what do you think? Do you believe every screen can truly be an Xbox? Let’s chat about your thoughts in the comments below! Don't forget to like and subscribe for more gaming updates, and until next time, happy gaming!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Neuralink Eye Implant Restores Vision to Blind People</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.wired.com/story/science-corporation-neuralink-eye-implant-restored-vision-blind-people/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A groundbreaking eye implant developed by Science Corporation has reportedly restored partial vision in legally blind participants, allowing them to engage in activities such as reading and recognizing faces. This retinal implant, known as the Prima, uses a camera-equipped pair of glasses to relay visual information to a small chip implanted under the retina. The preliminary study shows that participants experienced significant improvements in vision acuity after a year of use, indicating a potential new avenue for treating age-related macular degeneration, a leading cause of vision impairment in older adults.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we've got some groundbreaking news from the world of biotechnology that could change the lives of millions. Imagine being legally blind and then, thanks to an innovative new implant, being able to read a book, play cards, or even recognize the faces of your loved ones. Yes, you heard that right! Science Corporation has announced their preliminary results from a clinical trial of the Prima eye implant that has restored partial vision in several blind participants. This small 2-mm square chip is surgically placed under the retina and works in conjunction with a specialized pair of glasses that captures visual data and converts it to electrical signals sent to the brain. What’s really fascinating is that this implant has reportedly allowed participants to improve their vision acuity significantly after just a year! Many went from an average acuity of 20/450 to around 20/160, and some even achieved vision scores as good as 20/63. While the vision may not be quite 'normal' and lacks color, the potential here is huge. People with geographic atrophy, a severe form of age-related macular degeneration, are particularly affected by this condition, and this new technology might just be the breakthrough we've been waiting for. While the details are still being worked out—like how often participants used the device's zoom feature to assist their vision—the overall feedback is overwhelmingly positive. If you or someone you know is affected by AMD, this could be a game changer in the near future. So be sure to subscribe and hit the bell for updates on this story, as the implications for further research and real-world applications could be monumental. Let me know your thoughts in the comments below! Could this be the future of vision restoration? I’ll catch you all in the next update!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Apple’s Sales in China Are Stalling. What Will It Sacrifice to Turn Things Around?</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://www.wired.com/story/apples-sales-in-china-are-stalling-what-will-it-sacrifice-to-turn-things-around/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple is facing significant challenges in China as iPhone sales decline and competition from local brands intensifies. The company’s AI initiative, Apple Intelligence, is hindered by strict regulatory requirements, preventing its launch in China and complicating its growth prospects. Despite maintaining a solid customer base, Apple must navigate cautious consumer sentiment and local regulations, leading to potential compromises that could affect its operations in the world's largest smartphone market. The recent opening of a research center in Shenzhen might signal Apple's intent to maintain ties in China while adapting to the local market dynamics.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we’re diving into some big news regarding one of the biggest players in the tech industry: Apple. So, grab your favorite snacks and buckle up because we’re talking about Apple’s rocky road in China! It seems that Apple is losing its grip in what was once its most lucrative market. Recent figures show a worrying decline in iPhone sales, and while Apple has managed to claw its way back into the top five smartphone manufacturers, it’s still losing significant market share to domestic giants like Huawei and Xiaomi. In spite of this, Apple’s stock has hit an all-time high. Can you believe that? Why the contradiction? Well, the market is buzzing with the belief that their new AI feature, Apple Intelligence, would be the key to rejuvenating sales by attracting more customers into upgrading their devices. But hold your horses! There’s a big hiccup in that plan—Apple Intelligence can’t launch in China due to strict regulatory requirements around AI, particularly those concerning its reliance on ChatGPT which, by the way, has been banned in China since early 2023. This could mean significant sacrifices for Apple moving forward! Tim Cook himself has spoken on the need for better engagement with regulatory bodies in both Europe and China. Recently, Apple made a bold move by opening its biggest research center outside of the U.S. in Shenzhen, likely trying to mend fences after moving a good chunk of iPhone production to India. Analysts see this as a way to regain favor and tackle the daunting challenges posed by changing consumer sentiments and the resurgence of local competitors like Huawei. And let’s be honest, consumers in China often view domestic brands as better value and alternatives worth considering. With regulations tightening and a landscape filled with hurdles, Apple may have to compromise in ways that seem uncharacteristic of a leading U.S. company. As we’ve seen, Apple’s past deals with local partners to navigate China’s legislative maze make it clear that moving forward, they’d need to strike local partnerships to create tailored services. It brings to mind an interesting question—are we looking at a future where the American tech giant becomes too embedded in the policies and whims of the Chinese state? It’s a complicated situation for sure, and while Apple continues to show commitment to the Chinese market, the stakes are higher than ever. So, what do you think? Will Apple manage to turn things around in China, or are these challenges just too great? Let me know your thoughts in the comments below! And as always, don’t forget to hit that like button and subscribe for more updates on tech news! Catch you in the next one!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>For the first time, beloved IDE JetBrains Rider will be available for free</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://arstechnica.com/gadgets/2024/10/for-the-first-time-beloved-ide-jetbrains-rider-will-be-available-for-free/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JetBrains Rider, a popular integrated development environment (IDE), is now available for free for the first time ever, making it accessible for hobbyists, open-source developers, and educators. This change is permanent and comes after JetBrains tested non-commercial licenses for other products. With Visual Studio ceasing support for macOS, Rider’s free availability presents an opportunity for macOS developers who need a robust IDE. However, users under the free license cannot opt out of anonymous usage statistics collection, and those with evolving project intentions may need to reassess their license eligibility.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What's up everyone, welcome back to the channel! I've got some exciting news for all you developers out there. JetBrains Rider, the beloved integrated development environment, or IDE, has just taken a huge step forward by making its software available for free, and this is not just a temporary thing—it’s permanent! That's right, for the first time ever, you can use Rider for free, opening the doors for hobbyists, open-source developers, and students who want to dive into coding without paying an annual fee. JetBrains has been testing out non-commercial licenses for some time now on other products, and they’ve finally decided to roll this out for Rider as well. They recognize that a significant number of developers, about 68% according to a Stack Overflow survey, actually code outside of work just for fun. So it makes perfect sense to give these users, as well as educators, a tool to work with without the commercial strings attached.\n",
      "\n",
      "Now, why is this a big deal? Well, for macOS developers, it’s particularly great news because Microsoft recently discontinued support for Visual Studio on macOS. Sure, there are alternatives like VS Code and Xcode, but many developers found Visual Studio to be more robust for specific types of projects. Rider could fill that gap perfectly! However, there is a little catch to this free ride. Users on the non-commercial license cannot opt out of that anonymous usage statistics collection—so if that’s a dealbreaker, you may want to think twice.\n",
      "\n",
      "Furthermore, JetBrains has made it clear that if you start a project without commercial intent but it later evolves into something that you’re looking to monetize, you might have to switch licenses. It's a gray area, but generally, if your project's intentions change, make sure you reassess your license requirements.\n",
      "\n",
      "So, overall, it's a big win for the development community, with JetBrains Rider becoming more accessible and possibly growing its user base drastically. I mean, how many of you have been wanting to learn coding or maybe working on an open-source project? Now, you can use a top-notch IDE without breaking the bank! Let me know your thoughts in the comments below, and if you haven’t subscribed yet, make sure to hit that button for all the latest updates in tech and development. Thanks for watching, and I’ll catch you in the next one!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>US copyright office frees the McFlurry, allowing repair of ice cream machines</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://arstechnica.com/tech-policy/2024/10/us-copyright-office-frees-the-mcflurry-allowing-repair-of-ice-cream-machines/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The US Copyright Office has granted an exemption allowing for the repair of soft-serve ice cream machines, such as those used in McDonald's, which previously faced technological barriers due to digital locks. Consumer advocacy group Public Knowledge collaborated with iFixit to petition for this exemption, aiming to eliminate challenges in diagnosing and fixing these machines. The ruling is seen as a significant win for franchise owners and independent repair shops, encouraging third-party repair efforts while addressing long-standing issues with accessing error codes and necessary repairs.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we've got some really sweet news coming from the US Copyright Office that you’re going to love, especially if you’ve ever been frustrated trying to get your favorite McFlurry fix! In a recent ruling, they’ve granted an exemption that allows for the repair of soft-serve ice cream machines, like the infamous ones used at McDonald’s. Now, if you’ve ever tasted the disappointment of finding out those machines are down, you’ll understand why this is a big deal. Many consumers and even franchise owners had faced a hassle due to the digital locks protecting the machine's software that made repairs tricky. Thanks to the advocacy work by groups like Public Knowledge and iFixit, who pushed for this change, they have opened the door for easier maintenance and repairs of these machines. This indicates not only a victory for ice cream lovers but also for small business owners and independent repair shops who often struggled with outdated manuals and complex error codes that made diagnosing issues nearly impossible. It’s no secret that those cryptic error codes could leave you scratching your head and feeling completely hopeless, especially when there were legal barriers against circumventing those locks. But now, with the exemption in place, it’s like the digital ice cream gates have swung wide open! Meredith Rose, the Senior Policy Counsel over at Public Knowledge, even referred to this win with a sprinkle of optimism, saying it could lead to a flurry of third-party repair activity, ultimately helping businesses serve customers better. While the exemption doesn't cover commercial and industrial food prep devices, it’s a step towards making the right to repair a reality for more users out there. This ruling is particularly relevant in light of a lawsuit involving McDonald’s and a company called Kytch, who provided tools to help manage the ice cream machines. They are currently working on a settlement, and it's an exciting time for those who want to see more accessible repairs in the food service industry. So there you have it! The repair revolution is sweetening things up across fast-food chains, and who knows, this might just open up even more discussions about the right to repair in various industries. If you found this news interesting or have ever faced a broken McFlurry machine, drop your thoughts in the comments below! Don't forget to like, subscribe, and hit that notification bell for more updates on tech news that impacts our daily lives. Until next time, keep those ice cream dreams alive!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Tesla's Q3 buoyed by credits, cost-cutting as auto revenue stays flat</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://arstechnica.com/cars/2024/10/teslas-q3-buoyed-by-credits-cost-cutting-as-auto-revenue-stays-flat/>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tesla reported a strong third quarter in 2024, making a profit of $2.2 billion, supported by increased regulatory credits and lower production costs. Despite flat automotive sales, Tesla delivered 6% more vehicles year over year, with revenue from the automotive sector growing by 2%. The company saw remarkable growth in its battery and solar operations by 52%, and services revenue increased by 29%. Total revenue rose to $25.2 billion, while gross profit jumped 20%. However, concerns linger about an aging product lineup, with future growth dependent on new vehicle platforms and potential regulatory challenges.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone, welcome back to the channel! Today, we're diving into some exciting news from Tesla as they just announced their third-quarter earnings for 2024, and spoiler alert—they're looking pretty good! So, let's break it down. Tesla reported an impressive $2.2 billion in net profit for Q3. You heard it right! This comes off the back of a strong return after what many considered a rocky first half of the year. In fact, they delivered 6% more electric vehicles than the same time last year, which is a solid indicator that they're bouncing back! Now, you might be wondering about the automotive revenue. It did grow, but just by 2%, bringing in around $20 billion. However, Tesla's real magic seems to be with their other segments. They had an amazing 52% growth in their battery and solar operations, raking in $2.4 billion. Plus, revenue from services also saw a nice jump, up 29%, to $2.8 billion. All of this helped boost their total revenue to $25.2 billion, an 8% increase from last year. And check this out—their gross profit surged by 20%, topping $5 billion! Now, a significant factor in these numbers is Tesla's cost-reduction initiatives. They managed to cut operating expenses by 6%, which, let's be honest, is no small feat. They’ve also been getting creative with earnings from their Supercharger network and selling emissions credits, netting about $739 million in Q3 from that alone. But what about the future? Tesla is still in a bit of a tricky spot; they’ve acknowledged that their product lineup is a bit stale at the moment. So, while they have some innovations on the horizon—potentially based on a new vehicle platform—they’re focusing on utilizing existing manufacturing capacity in places like California and Texas. They even teased the arrival of a new two-seat CyberCab coming before 2027! Sounds promising, right? But there are regulatory concerns looming. If federal regulators mandate a big hardware recall related to Tesla's autonomous driving system, it could dent their bright expectations. So, there you have it—Tesla is showing signs of recovery and growth, even while navigating a tough market landscape. I’m excited to see what’s coming next for them! Don’t forget to like, subscribe, and hit that notification bell to stay updated on the latest in the tech world. See you next time!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Scientists Grow Crops in Near Total Darkness Thanks to New Electro-Agriculture Technique</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://gizmodo.com/scientists-grow-crops-in-near-total-darkness-thanks-to-new-electro-agriculture-technique-2000515512>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scientists have developed a groundbreaking technique called 'electro-agriculture' that allows crops to grow in near-total darkness, potentially revolutionizing farming and combatting global food insecurity. This new method bypasses traditional photosynthesis, enhancing crop yields without relying on sunlight or increasing environmental impact. By utilizing acetate created from carbon dioxide and water, this system could lead to substantial agricultural efficiencies, with researchers achieving promising results in growing various crops like lettuce and tomatoes under these conditions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone! Welcome back to the channel, where we dive into the latest breakthroughs in science and technology! Today, I've got some astonishing news that could change the future of farming as we know it. Researchers have developed a revolutionary technique called 'electro-agriculture,' which allows crops to thrive in near-total darkness! Yes, you heard that right—darkness! This fascinating method aims to address the dire issue of food insecurity faced by billions worldwide. As the population continues to grow and climate change poses more challenges, scientists are looking for more efficient ways to produce food—and that’s where electro-agriculture comes into play. Traditionally, plants rely on photosynthesis to convert sunlight into energy, but the researchers at the University of California, Riverside found that this process is just not efficient enough for our needs. So, what did they do? They developed a new approach that bypasses conventional photosynthesis, allowing them to cultivate crops without the constraints of sunlight! Imagine the possibilities! With this technology, agriculture could be decoupled from the environment, and food can be grown in controlled indoor environments. The researchers reported that they have already seen preliminary success with various crops like lettuce, rice, and tomatoes. The secret behind this innovation lies in the use of acetate—created from carbon dioxide and water—which aids in boosting plant metabolism. It gets even better: this technique could lead to a staggering reduction in land needed for food production, by up to 94%! Now, I can almost hear the skeptics questioning: how is this even possible? Well, electro-agriculture operates in such a way that plants require very little light—definitely far less than you'd think! Additionally, this process significantly reduces the carbon footprint associated with food production, which is a huge win for the environment. As we tackle the pressing issue of hunger and rising food costs, this method could be the key to making food more accessible for everyone. So, could tomatoes grown in the dark be the next big thing? It looks like researchers believe so! Stay tuned for more updates on this groundbreaking technology as we continue to uncover the incredible advancements in our agricultural future!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>NASA’s New Lightning Satellite Just Captured Some Terrifyingly Wild Footage</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://gizmodo.com/nasas-new-lightning-satellite-just-captured-some-terrifyingly-wild-footage-2000513637>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The National Oceanic and Atmospheric Administration (NOAA) has released stunning imagery captured by its new satellite, GOES-19, which is equipped with a Geostationary Lightning Mapper. This tool monitored lightning during the formation of Hurricanes Helene and Milton, providing unprecedented data that could enhance storm prediction models and improve flight safety. GOES-19, launched in June and in testing phase, is expected to fully operational by April 2025, replacing GOES-16. Furthermore, NOAA and NASA are collaborating on a next-gen satellite system called GeoXO, set to launch in the early 2030s, with even more advanced weather monitoring capabilities.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What’s up, everyone! Today we're diving into some incredible weather tech news that’s sure to blow your mind! The National Oceanic and Atmospheric Administration, or NOAA for short, just unveiled stunning footage from its latest weather satellite, the GOES-19, which is officially shaking up how we monitor storms and lightning from space! This satellite is equipped with cutting-edge technology known as the Geostationary Lightning Mapper, and it's already been put to the test during the formation of Hurricanes Helene and Milton! You heard that right; this satellite was capturing breathtaking imagery of hurricane lightning in real-time!\n",
      "\n",
      "Imagine powerful hurricanes ripping through the United States, and we get to watch as lightning illuminates the storm clouds from above! This technology allows scientists to observe storms in unprecedented detail, helping to enhance our understanding of how hurricanes form and evolve. NOAA has pointed out that the data collected could play a key role in developing new models for storm analysis and prediction. This means improved safety for flights navigating over oceans that lack radar coverage. How cool is that?\n",
      "\n",
      "Now, let's talk about what makes this footage so captivating; while there’s devastation on the ground, the images from space are jarringly beautiful. You can see erratic lightning flickering like fireflies and vivid patterns forming across the storm clouds. It's like nature’s own light show!  GOES-19 was launched back in June aboard a SpaceX Falcon Heavy rocket, and while it's still in the testing phase, it’s anticipated to be fully operational by April 2025, stepping in as the new GOES-East satellite.\n",
      "\n",
      "But wait, there’s more! NOAA and NASA are already planning the future with something called the Geostationary Extended Observations or GeoXO system, aiming to revolutionize weather monitoring even further by the early 2030s! This next-gen system is expected to be equipped with groundbreaking tools that will track everything from lightning and air quality to extreme weather events. Stay tuned, folks, because this technology is set to transform how we understand and navigate our planet's weather patterns! Don’t forget to like and subscribe for more updates on this and other fascinating tech news!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h1>Meet the 99-Ton Robot That’s Printing Rocket Lab's Next-Gen Neutron Rocket</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: <https://gizmodo.com/meet-the-99-ton-robot-thats-printing-rocket-labs-next-gen-neutron-rocket-2000514991>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Summary</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rocket Lab has unveiled a groundbreaking 99-ton, 39-foot tall custom-built 3D printing robot at its Maryland manufacturing complex, which is set to significantly speed up the production of its Neutron rocket by laying down carbon composite materials at a remarkable speed. This world-first automated fiber placement machine is designed to construct the largest structures of the Neutron rocket, including its upper stage domes and fuel tanks, potentially saving over 15,000 hours of manufacturing time. Rocket Lab CEO Peter Beck expressed excitement for the innovative technology, which will not only be used for the Neutron but also for other projects, positioning Rocket Lab to compete with industry giants in a rapidly evolving aerospace sector.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suggested Text to Say</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey everyone! Welcome back to the channel! Today we’re diving into some exciting news from the world of space technology. Rocket Lab, a key player in the aerospace industry, has just rolled out a colossal 99-ton, 39-foot robot that could redefine how rockets are built! This isn't just any robot; it's the largest of its kind in the world and is capable of 3D printing components for Rocket Lab's upcoming Neutron rocket. Imagine this, folks: this beast can lay down carbon fiber composite layers at a jaw-dropping speed of 328 feet per minute! That's like finishing a task that would typically take weeks in just ONE day! Talk about efficiency! \n",
      "\n",
      "So, what does this mean for Rocket Lab? Well, the Neutron rocket is designed to be a medium-lift launch vehicle with a reusable first stage, allowing it to deliver hefty payloads to low Earth orbit. Thanks to this cutting-edge 3D printer, Rocket Lab is aiming to save a whopping 15,000 hours in manufacturing time! Rocket Lab's CEO, Peter Beck, is buzzing with enthusiasm over this revolutionary technology, stating that it’s crucial for building the world’s largest carbon composite rocket. \n",
      "\n",
      "But wait, there’s more! This 3D printer isn’t just going to work exclusively on the Neutron. Rocket Lab plans to use this technological marvel for their Electron rocket and various components for spacecraft, including solar panel substrates. The aerospace industry is stepping into a new era, and 3D printing is at the helm, paving the way for cost-effective and reusable launch vehicles. It's a method that hopes to challenge big names like SpaceX in the commercial space race. \n",
      "\n",
      "So, what do you all think? Are we ready for the future of space travel powered by robots and 3D printing? Drop your thoughts in the comments below, and don’t forget to like and subscribe for more awesome updates from the tech and space world! Until next time, keep your heads in the stars!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title, details in final_text.items():\n",
    "    display(Markdown(f'<h1>{title}</h1>'))\n",
    "    print(f\"Source: <{details[1]['url']}>\")\n",
    "    \n",
    "    if details:\n",
    "        detail = details[0]\n",
    "        \n",
    "        display(Markdown(\"<h4>Summary</h4>\"))\n",
    "        print()\n",
    "        print(detail['resume'])\n",
    "        print()\n",
    "        \n",
    "        display(Markdown(\"<h4>Suggested Text to Say</h4>\"))\n",
    "        print()\n",
    "        print(detail['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb014086-0ed7-493a-925b-d703c9dfc9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
